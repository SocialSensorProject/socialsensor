%!TEX root = icwsm2017.tex

\label{sec:introduction}
\COMMENT
Social media sites such as Twitter present a double-edged sword for
users.  On one hand these sources contain a vast amount of novel and
topical content that challenge traditional news media sources in terms
of their timeliness and diversity.  Yet on the other hand they also
contain a vast amount of chatter and otherwise low-value content for most
users' information needs where filtering out irrelevant content is
extremely time-consuming.  Previous work~\cite{lin2011smoothing,yang2014large,magdy} 
has noted this need for topic-based filtering and has adopted a range
of variations on supervised classification techniques to build effective
topic filters.

While these previous approaches have augmented their respective topical classifiers with extensions
ranging from semi-supervised training to multiple stages of classification-based filtering to online tracking of foreground and background language model evolution, 
% --- all critical for state-of-the-art performance ---
we seek to analyze the lowest common denominator
of all of these methods, namely the performance of the underlying (vanilla) supervised 
classification paradigm.
Our fundamental research questions in this paper are hence focused on a longitudinal study
of the performance of such supervised topic classifiers.  
%taken a variety
%of approaches ranging from information retrieval and language models~\cite{lin2011smoothing}  
%to topic modeling~\cite{yang2014large} to supervised classification~\cite{magdy}.
For example, over a year or more after training, how well do such classifiers generalize to future novel topical content, and are such results stable across a range of topics?  Furthermore, what features, feature classes, and feature attributes are most critical for long-term classifier performance?  

To answer these questions, we collected a corpus of over 800 million English Tweets via the Twitter streaming API during 2013 and 2014 and learned topic classifiers for 10 diverse themes ranging from social issues to celebrity deaths to the ``Iran nuclear deal''.  We leverage ideas from~\cite{lin2011smoothing} for curating hashtags to define our 10 training topics and label
tweets for supervised training; however, we also curate a disjoint set of test hashtags and 
% to test the generalization performance of the topic classifiers
\emph{and use only these to label test data} (because training hashtags can be trivially memorized) to test true generalization performance of the topic filters to future novel content. 

We empirically show that two simple and efficiently trainable methods ---
logistic regression and naive Bayes --- generalize well to unseen
future topical content (including content with no hashtags) in terms
of their average precision (AP) and Precision@$n$ for a range of
$n$ evaluated over long time-spans (typically one year or more).  
Furthermore, we show that terms and locations are among the most
useful features --- surprisingly more so than hashtags, even though
hashtags were used to label the data.  And perhaps even more
surprisingly, the number of unique hashtags and tweets by a user
correlates more with their informativeness than their follower or
friend count.  

In summary, this work provides a longitudinal study of Twitter topic classifiers that further justifies supervised approaches used in existing work while providing detailed insight into feature properties critical for their performance.

%Twitter hosts lots of information, on average more than $2,200$ new tweets every second. This can get up to $3$ to $4$ times increase during large events such as tsunami. \footnote{\hyperref[]{https://blog.twitter.com/2011/the-engineering-behind-twitter-s-new-search-experience}}
%\begin{itemize}
%\item Twitter is a vast sensor of content generated by latent phenonema (e.g., flu, political sentiment, elections, environment).
%\item Learning topical social sensors (politicians in NY, road conditions in Toronto) -- very broad topics for which its hard to manually specify a useful query.
%\item But there is interesting topical content and wouldn't it be cool if we could learn a social sensor for a targeted topic?
%\item Key insight is that hashtags are topical and can be used to bootstrap a supervised learning system that as we will show generalizes well beyond the seed hashtags.
%\item Conclusion is a new way to build topical real-time feeds that are otherwise difficult to do with existing Twitter tools (???).
%\end{itemize}
%section{Learning Topical Social Sensors}

%Start off with the questions that we want to answer in this section:
%
%- How to evaluate, labeling (problem of no supervised labels for tweets, indirect via hashtags as topical surrogates, leads to question of hashtag curation)?
%
%- Which classification algorithm is best / most robust for learning topical social sensors?
\ENDCOMMENT
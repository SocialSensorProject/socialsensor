%!TEX root = icwsm2017_poster.tex

% This section covers the *formal* framework for learning topical social sensors

% =====================
% What are we doing?  
% (1) Formal problem setup and notation definitions.  Corpus of documents, features, labels, classification problem definition (for a generic classifier).
% (2) How we label data.
% (3) How we train (validation set critical for hyperparameters, what metric used for selection?).
% Note that formal performance evaluation provided in experimental section.
% =====================

% (1) Formal problem setup and notation definitions.  Corpus of documents, features, labels, classification problem definition (for a generic classifier).
%TODO: Formal learning framework.
Our objective is to build a binary classifier that can label
a previously unseen tweet as topical (or not) by training on 
topically labeled historical tweets.  
%
%Formally, 
%given an arbitrary tweet $d$ (a document in text classification parlance) 
%and a set of topics $T = \{
%t_1,\ldots,t_K\}$, we wish to train a scoring function $f^t: D \rightarrow \mathbb{R}$
%for topic $t \in T$ over a subset of labeled training tweets from $D = \{
%d_1,\ldots,d_N \}$.  Each $d_i \in D$ has a boolean feature vector 
%$(d_i^1,\ldots,d_i^M) \in \{0,1\}^M$.  A boolean function $t : D \rightarrow \{
%0,1 \}$ indicates whether the tweet $d_i$ is topical (1) or not
%(0).  In an ideal scenario, we could train for topic $t$ 
%so that $\forall d_i \; f^t(d_i) = t(d_i)$ --- our scoring
%function (or more generally some threshold on it) agrees perfectly with the topic
%labels of all tweets.  
%
%A critical bottleneck for learning targeted topical social classifiers 
%is to achieve sufficient supervised content labeling.  With data
%requirements often in the thousands of labels to ensure effective
%learning and generalization over a large candidate feature space (as
%found in social media), manual labeling is simply too time-consuming
%for many users, while crowdsourced labels are both costly and prone to
%misinterpretation of users' information needs.  Fortuitously, hashtags
%have emerged in recent years as a pervasive topical proxy on social
%media sites --- hashtags originated on IRC chat, were adopted later
%(and perhaps most famously) on Twitter, and now appear on other social
%media platforms such as Instagram, Tumblr, and Facebook.  
%
Following
the approach of~\cite{lin2011smoothing}, for each topic $t \in T$, we leverage a (small) set of
user-curated topical hashtags $H^t$ to efficiently provide a large number of
supervised topic labels for social media content.  
%Next we provide
%a procedure for labeling data with $H^t$ for training and validation.
%Following this, we proceed to train
%supervised classification and ranking methods to learn topical content
%from a large feature space (e.g., this feature space
%includes terms, hashtags, mentions, authors and their locations). The
%training process involves two steps:
%\begin{enumerate}
%\item {\bf Temporally split train and validation using $H^t$:}
As standard for machine learning methods, we divide our training data into
train and validation sets --- the latter for hyperparameter tuning to control
overfitting and ensure generalization to unseen data.  
As a critical insight for topical generalization where we view correct classification 
of tweets with \emph{previously unseen topical hashtags} as a proxy for topical generalization, 
we \emph{do not} simply
split our data temporally into train, validation, and test sets and label both with \emph{all} 
hashtags in $H^t$.  \emph{Instead},
we split $H^t$ into three disjoint sets $H^t_\mathrm{train}$, $H^t_\mathrm{val}$, and 
$H^t_\mathrm{test}$ 
according to two time stamps $t^\mathrm{val}_\mathrm{split}$ and $t^\mathrm{test}_\mathrm{split}$ for topic $t$ and the first usage time stamp 
$h_\mathrm{time*}$ of each hashtag $h \in H^t$.  In short, all hashtags $h \in H^t$ with
$h_\mathrm{time*} < t^\mathrm{val}_\mathrm{split}$ are used to generate positive labels in the training data, those with $h_\mathrm{time*} \geq t^\mathrm{test}_\mathrm{split}$ are used to generate positive labels in the test data and the remainder are used to label the validation data.

%To achieve this effect formally, we define the following:
%\begin{align*}
%H^t_\mathrm{train} & = \{ h | h \in H^t \land h_\mathrm{time*} <    t_\mathrm{split} \} ,  \\
%H^t_\mathrm{val}   & = \{ h | h \in H^t \land h_\mathrm{time*} \geq t_\mathrm{split} \} .
%\end{align*}
%Once we have split our hashtags into training and validation sets
%according to $t_\mathrm{split}$, we next proceed to temporally split
%our training documents $D$ into a training set $D^t_\mathrm{train}$ and a validation set
%$D^t_\mathrm{val}$ for topic $t$ based on the posting
%time stamp $d_{i,\mathrm{time*}}$ of each tweet $d_i$ as follows: 
%%ormally, given $H^t$, we
%%can label each document $d_i$ (containing positive features $D_i^+$) as follows:
%\begin{align*}
%D^t_\mathrm{train} & = \{ d_i | d_i \in D \land d_{i,\mathrm{time*}} <    t_\mathrm{split} \} ,  \\
%D^t_\mathrm{val}   & = \{ d_i | d_i \in D \land d_{i,\mathrm{time*}} \geq t_\mathrm{split} \} .
%\end{align*}
%Next we define the set of positively occurring features for a document
%$d_i$ formally as $D_i^+ = \{ j | d_i^j=1 \}_{j=1\ldots M}$ and note that
%$D_i^+$ may include feature IDs for the content of $d_i$ (e.g., terms and, importantly, 
%hashtags) as well as its meta-data (e.g., author, location).
%Then to label both the train and validation data sets $D^t_\mathrm{train}$ and $D^t_\mathrm{val}$, 
%we use the respective
%hashtag sets $H^t_\mathrm{train}$ and $H^t_\mathrm{val}$ for generating
%the topic label for a particular tweet $t(d_{i}) \in \{0,1\}$ as follows:
%\begin{align*}
%t(d_{i}) & =
%  \begin{cases}
%    1: \exists_{h \in H^t_\mathrm{train}} \; h \in D_i^+ \land d_{i} \in D^t_\mathrm{train} \\
%    1: \exists_{h \in H^t_\mathrm{val}}   \;\;\; h \in D_i^+ \land d_{i} \in D^t_\mathrm{val}   \\
%    0: \mathrm{otherwise}
%  \end{cases} .
%\end{align*}

%To recap the methodology, we specify a set of hashtags as a proxy for a topic.  We split
%those hashtags into train and validation sets according to a time of first usage and fixed time point.
%We split the tweet data into train and validation sets according to the same fixed time point.
%Finally, we label a tweet in the train (validation) data set as positive if it contains
%a hashtag in the train (validation) hashtag set.  

The critical insight here is that we not only partition the train, validation, and test data 
temporally, but we also divide the hashtag labels temporally and label each data partition with
an entirely disjoint set of topical hashtags.
%Selection of a set of documents and temporally splitting them into
%train and validation documents. The split is based on a split-time
%defined on hashtag set $H^{t}$ to preserve enough number of hashtags
%in train and validation sets.
The purpose behind this training and validation data split
and labeling is to ensure that learning hyperparameters are tuned so as
to prevent overfitting and maximize generalization to unseen topical
content (i.e., new hashtags).
We remark that a classifier that simply
memorizes training hashtags will fail to correctly classify the validation data except in 
cases where a tweet contains both a training and validation hashtag.  

Next we proceed to train our classifier and  
%scoring function $f^t$ on $D^t_\mathrm{train}$ and
select hyperparameters to optimize Average Precision (AP)~\cite{manning_ir} (a ranking
metric) on the validation data (and hashtag labels).
Because all of our classifiers provide a real-valued score (e.g., logistic regression
provides the probability of the class), the classifiers can be used to rank.
In our results, we report final evaluations on the held-out test data with their own
disjoint set of topical hashtag labels.

%Analogously to train and validation data, test data is generated the same way, but 
%we omit formal notation (identical to the above) in order to reduce clutter.
%Test data has its own time stamp ($> t_\mathrm{split}$), its own set of
%temporally disjoint data, and its own set of hashtags 
%temporally disjoint from the training and validation data; this is done in order to evaluate
%generalization performance of the learned classifier on future data with topical hashtags
%not seen during training.    
%With train, validation, and testing data defined along with the training methodology,
%it remains now to empirically evaluate and analyze classifier performance, 
%described next.

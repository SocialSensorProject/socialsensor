% This section covers the *formal* framework for learning topical social sensors
% =====================
% What are we doing?  
% (1) Formal problem setup and notation definitions.  Corpus of documents, features, labels, classification problem definition (for a generic classifier).
% (2) How we label data.
% (3) How we train (validation set critical for hyperparameters, what metric used for selection?).
% Note that formal performance evaluation provided in experimental section.
% =====================
% (1) Formal problem setup and notation definitions.  Corpus of documents, features, labels, classification problem definition (for a generic classifier).
%TODO: Formal learning framework.
In this section, we reduce the problem of learning topical content from large space of features and small set of examples provided by the user to the following setting that will match standard supervised learning paradigm.

Here, the problem statement is that the user has an information need for high-precision topical content from Twitter. 
The first step for the user is that he/she must provide labeled data to represent this information need for use in a targeted supervised learning setting.
 We assume that for each topic, the user will provide us with a set of hashtags. For example for the topic of \textit{Natural Disaster}, 
the user will give us \{\#earthquake,\#flood,\#prayforthephilippines, ...\}. The goal is, given this topic and related hashtags, return a ranked
 list of tweets to the user that are highly relevant to the topic and match users information needs; meaning instead of returning a set of tweets
 that only match "disaster" or "natural", we realize the actual information need behind the searched topic and present all tweets matching this need. 
To this end, we need a methodology that learns from the set of hashtags provided by users on how to pick up sensors (i.e., useful terms, mentions, 
hashtags, locations, and users) and weight them to ensure picking new, unseen topical hashtags in future tweets. The following discussion intends
 to answer how to develop such methodology. %To this end, we assume a birth-time for each hashtag as the first time it has been used in our dataset. Assuming a birth-time, then we consider temporal split of our chosen set of hashtags and learn on one set and test on the other more recent set to evaluate the methods generalizability. The limitation of this method is since we use hashtags as topical proxies, we mislabel the tweets without hashtags. Therefore, we miss part of topical content that we could learn from for the price of automatic labeling of all tweets with minimal user effort. Now, we move on to mathematically formalize this problem.}

Our objective in learning social sensors is to train an automatic
system for ranking documents by their topical relevance.  Formally,
given an arbitrary document $d$ and a set of topic classes $C = \{
c_1,\ldots,c_K\}$, we wish to train a scoring function $f\!\!:\!\!d \rightarrow \mathbb{R}$
over a set of training documents $D = \{
d_1,\ldots,d_N \}$.
Considering $M$ number of features extracted from dataset, each document $d_i \in D$ has a boolean feature vector
$(d_i^1,\ldots,d_i^M) \in \{0,1\}^M$ and boolean label $d_i^c \in \{
0,1 \}$ indicating whether the document $d_i$ is topical (1) or not
(0).  We define the set of positively occurring features for a document
$d_i$ as $D_i^+ = \{ d_i^j | d_i^j=1 \}_{j=1\ldots M}$ and note that
$D_i^+$ may include features for the content of $d_i$ (e.g., terms, 
hashtags) as well as its meta-data (e.g., author, location).

There are two catches that make our training setting somewhat
non-standard and which underlie subtle but critical contributions in this
work:  
\begin{enumerate}
\item Manually labeling documents is time-consuming so
we need a way to label a large number of tweets with minimal
user curation effort; \emph{We achieve this by using hashtags as topical proxies.}
\item We need to train our social sensor on
known topical content, but tune it on novel topical validation content 
that ensures the tuning achieves optimal generalization; \emph{We achieve this by 
excising training content from our validation data so that our scoring
function hyperparameter tuning ensures generalization.}
\end{enumerate}
We next explain these key innovations in detail.

%\begin{equation}
%(\gamma, M) : D \to T 
%\end{equation}
%
%\begin{equation}
%t^{*} = argMin_{w} L(t,\hat{t})
%\end{equation}
%
%Where ${L : T \times T \to \Re_{+} }$ is the loss function indicating the penalty for an incorrect prediction and ${L(t,\hat{t})}$ is the loss for prediction of ${\hat{t}}$ instead of actual topic $t$.

%SCORING TWEETS AT TEST TIME
%Each document ${d_{i}} \in D$ is scored for a given topic ${t \in \{ T \}}$ by the measure of it's similarity to the topic defined as
%
%\begin{equation}
%Sim({d_{i}}, t) = \sum_{j} F_{d_{i}}^{j}) \times {w_{j}}
%\end{equation}
%
%where $F_{d_{i}}^{j}$ represents the $j$th value in $F_{d_{i}}$.

% (2) How we label data.
A critical bottleneck for learning targeted topical social sensors
is to achieve sufficient supervised content labeling.  With data
requirements often in the thousands of labels to ensure effective
learning and generalization over a large candidate feature space (as
found in social media), manual labeling is simply too time-consuming
for many users and crowdsourced labels are both costly and prone to
misinterpretation of users' information needs.  Fortuitously, hashtags
have emerged in recent years as a pervasive topical proxy on social
media sites --- hashtags originated on IRC chat, were adopted later
(and perhaps most famously) on Twitter, and now appear on other social
media platforms such as Instagram, Tumblr, and Facebook.  Hence as a
simple enabling insight that serves as a catalyst for effective
topical social sensor learning, for each topic class $c \in C$, we leverage a (small) set of
user-curated topical hashtags $H^c$ to efficiently provide a large number of
supervised topic labels for social media content.  Next we will provide
the formal procedure for labeling data with $H^c$ and training.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure*}[t!]
%\centering
%\includegraphics[width=0.8\textwidth, height=50mm]{images/testTrainSplit.png}
%%\vspace{-2mm}
%\caption {The method for temporally splitting hashtags and tweets to train, validation, and test sets}
%\label{fig:testTrainSplit}
%\vspace{2mm}
%\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% (3) How we train (validation set critical for hyperparameters, what metric used for selection?).
With the data labeling bottleneck resolved, we proceed to train
supervised classification and ranking methods to learn topical content
from a large feature space (e.g., for Twitter, this feature space
includes terms, hashtags, mentions, authors and their locations). The
training process includes the following two steps:
%TODO: An simple enumeration of the training steps?
\begin{enumerate}
\item {\bf Temporally split train and validation using $H^c$:}
As usual for machine learning methods, we divide our training data into
train and validation sets --- the latter for hyperparameter tuning to control
overfitting and ensure generalization to unseen data.  
As a critical insight for topical generalization where we view identification
of previously unseen hashtags as a proxy for topical generalization, we do not simply
split our data temporally into train and test sets as usually done.  Instead,
we split $H^c$ into two disjoint sets $H^c_\mathrm{train}$ and $H^c_\mathrm{val}$ 
according to a time stamp $t_\mathrm{split}$ and the first usage time stamp 
$t_h^\mathrm{*}$ of hashtags $h \in H^c$.
 Formally, we define the following:
\begin{align*}
H^c_\mathrm{train} & = \{ h | h \in H^c \land t_{h}^* <    t_\mathrm{split} \} ,  \\
H^c_\mathrm{val}   & = \{ h | h \in H^c \land t_{h}^{*} \geq t_\mathrm{split} \} .
\end{align*}
Once we have split our hashtags into training and validation sets
according to $t_\mathrm{split}$, we next proceed to temporally split
our training documents $D$ into a training set $D^c_\mathrm{train}$ and a validation set
$D^c_\mathrm{val}$ for topic $c$ based on the posting
time stamp $t_{d_i}$ of each document $d_i$ as follows: 
%ormally, given $H^c$, we
%can label each document $d_i$ (containing positive features $D_i^+$) as follows:
\begin{align*}
D^c_\mathrm{train} & = \{ d_i | d_i \in D \land t_{d_i} <    t_\mathrm{split} \} ,  \\
D^c_\mathrm{val}   & = \{ d_i | d_i \in D \land t_{d_i} \geq t_\mathrm{split} \} .
\end{align*}
Then for each set of \emph{train} and \emph{val} tweets, we use the respective
hashtag sets $H^c_\mathrm{train}$ and $H^c_\mathrm{val}$ for labeling each $d_{i}^{c} \in D^c_\mathrm{train}$:
\begin{align*}
d_{i}^{c} & =
  \begin{cases}
    1: \exists_{h \in H^c_\mathrm{train}} \; h \in D_i^+ \\
    0: \mathrm{otherwise}
  \end{cases} .
\end{align*}
and similarly for each $d_{i}^{c} \in D^c_\mathrm{val}$:
\begin{align*}
d_{i}^{c} & =
  \begin{cases}
    1: \exists_{h \in H^c_\mathrm{val}} \; h \in D_i^+ \\
    0: \mathrm{otherwise}
  \end{cases} .
\end{align*}
The critical insight here is that we not only divide the train and validation
temporally, but we divide the hashtag labels temporally and label the validation
data with an entirely disjoint set of topical labels from the training data.
%Selection of a set of documents and temporally splitting them into
%train and validation documents. The split is based on a split-time
%defined on hashtag set $H^{t}$ to preserve enough number of hashtags
%in train and validation sets.
The purpose behind this training and validation data split
and labeling is to ensure that learning hyperparameters are tuned so as
to prevent overfitting and maximize generalization to unseen topical
content (i.e., new hashtags).
\item {\bf Training and hyper-parameter tuning:}
Once $D^c_\mathrm{train}$ and $D^c_\mathrm{val}$ have been constructed,
we proceed to train our scoring function $f$ on $D^c_\mathrm{train}$ and
select hyperparameters to optimize Average Precision (AP) on
$D^c_\mathrm{val}$.  Once the optimal $f$ is found for $D_\mathrm{val}$,
we return it as our final learned topical scoring function for topic $t$.
%\item {\bf Learning: The weight vector $W$ is learned with classification method $M$ on the selected set of documents using tuned hyper parameters
\end{enumerate}
% Note that formal performance evaluation provided in experimental section.
Having defined our topical social sensor learning paradigm, it now remains
to empirically evaluate this methodology in a social media setting, which
we describe next.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{table}[t!]
%\vspace{-0.5mm}
%\centering
%{\footnotesize
%%\small
%\renewcommand{\arraystretch}{1.2}
%\begin{tabular}{|l|l|}
%\hline
%\textbf{Meaning}  & \textbf{Notation}\\ \hline
%Topics & $C = \{c_1,\ldots,c_K\}$ \\ \hline
%Documents & $D = \{d_1,\ldots,d_N \}$ \\ \hline
%Set of hashtags of topic class $c$ & $H^c$ \\ \hline
%Set of train hashtags of topic $c$ & $H^c_\mathrm{train}$ \\ \hline
%Set of validation hashtags of topic $c$ & $H^c_\mathrm{val}$ \\ \hline
%Time of the first usage of hashtag $h \in H^c$ & $t_h^{*}$ \\ \hline
%Posting time of tweet $d_i$ & $t_{d_i}$ \\ \hline
%Set of training tweets of topic $c$ & $D^c_\mathrm{train}$ \\ \hline
%Set of validation tweets of topic $c$ & $D^c_\mathrm{val}$ \\ \hline
%Positive occurring features for document $d_i$ & $D_i^+ = \{ d_i^j | d_i^j=1 \}_{j=1\ldots M}$ \\ \hline
%Boolean label for document $d_i$ and topic $c$ & $d_i^c \in \{0,1 \}$ \\ \hline
%\end{tabular}
%}
%\vspace{-1mm}
%\caption{Notations used and their meaning}
%\label{table:notations}
%\vspace{-1.5mm}
%\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

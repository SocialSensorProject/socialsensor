% This section covers the *formal* framework for learning topical social sensors

% =====================
% What are we doing?  
% (1) Formal problem setup and notation definitions.  Corpus of documents, features, labels, classification problem definition (for a generic classifier).
% (2) How we label data.
% (3) How we train (validation set critical for hyperparameters, what metric used for selection?).
% Note that formal performance evaluation provided in experimental section.
% =====================

% (1) Formal problem setup and notation definitions.  Corpus of documents, features, labels, classification problem definition (for a generic classifier).
%TODO: Formal learning framework.
Our objective in learning social sensors is to train an automatic
system for ranking documents by their topical relevance.  Formally,
given an arbitrary document $d$ and a set of topics $T = \{
t_1,\ldots,t_K\}$, we wish to train a scoring function $f\!\!:\!\!d \rightarrow \mathbb{R}$
over a set of training documents $D = \{
d_1,\ldots,d_N \}$ where each $d_i \in D$ has a boolean feature vector
$(d_i^1,\ldots,d_i^M) \in \{0,1\}^M$ and boolean label $d_i^t \in \{
0,1 \}$ indicating whether the document $d_i$ is topical (1) or not
(0).  We define the set of positively occurring features for a document
$d_i$ as $D_i^+ = \{ d_i^j | d_i^j=1 \}_{j=1\ldots M}$ and note that
$D_i^+$ may include features for the content of $d_i$ (e.g., terms, 
hashtags) as well as its meta-data (e.g., author, location).

There are two catches that make our training setting somewhat
non-standard and which underlie subtle but critical contributions in this
work:  (1) Manually labeling documents is time-consuming so
we need a way to manually label a large number of tweets with minimal
user curation effort; \emph{We achieve this by using hashtags as topical proxies.}
(2) We need to train our social sensor on
known topical content, but tune it on novel topical validation content 
that ensures the tuning achieves optimal generalization; \emph{We achieve this by 
excising training content from our validation data so that our scoring
function hyperparameter tuning ensures generalization.}
We next explain these key innovations in detail.

%\begin{equation}
%(\gamma, M) : D \to T 
%\end{equation}
%
%\begin{equation}
%t^{*} = argMin_{w} L(t,\hat{t})
%\end{equation}
%
%Where ${L : T \times T \to \Re_{+} }$ is the loss function indicating the penalty for an incorrect prediction and ${L(t,\hat{t})}$ is the loss for prediction of ${\hat{t}}$ instead of actual topic $t$.

%SCORING TWEETS AT TEST TIME
%Each document ${d_{i}} \in D$ is scored for a given topic ${t \in \{ T \}}$ by the measure of it's similarity to the topic defined as
%
%\begin{equation}
%Sim({d_{i}}, t) = \sum_{j} F_{d_{i}}^{j}) \times {w_{j}}
%\end{equation}
%
%where $F_{d_{i}}^{j}$ represents the $j$th value in $F_{d_{i}}$.

% (2) How we label data.
A critical bottleneck for learning targeted topical social sensors
is to achieve sufficient supervised content labeling.  With data
requirements often in the thousands of labels to ensure effective
learning and generalization over a large candidate feature space (as
found in social media), manual labeling is simply too time-consuming
for many users and crowdsourced labels are both costly and prone to
misinterpretation of users' information needs.  Fortuitously, hashtags
have emerged in recent years as a pervasive topical proxy on social
media sites --- hashtags originated on IRC chat, were adopted later
(and perhaps most famously) on Twitter, and now appear on other social
media platforms such as Instagram, Tumblr, and Facebook.  Hence as a
simple enabling insight that serves as a catalyst for effective
topical social sensor learning, for each topic $t \in T$, we leverage a (small) set of
user-curated topical hashtags $H^t$ to efficiently provide a large number of
supervised topic labels for social media content.  Next we will provide
the formal procedure for labeling data with $H^t$ and training.

% (3) How we train (validation set critical for hyperparameters, what metric used for selection?).
With the data labeling bottleneck resolved, we proceed to train
supervised classification and ranking methods to learn topical content
from a large feature space (e.g., for Twitter, this feature space
includes terms, hashtags, mentions, authors and their locations). The
training process includes the following two steps:
%TODO: An simple enumeration of the training steps?
\begin{enumerate}
\item {\bf Temporally split train and validation using $H^t$:}
As usual for machine learning methods, we divide our training data into
train and validation sets --- the latter for hyperparameter tuning to control
overfitting and ensure generalization to unseen data.  
As a critical insight for topical generalization where we view identification
of previously unseen hashtags as a proxy for topical generalization, we do not simply
split our data temporally into train and test sets as usually done.  Instead,
we split $H^t$ into two disjoint sets $H^t_\mathrm{train}$ and $H^t_\mathrm{val}$ 
according to a time stamp $t_\mathrm{split}$ and the first usage time stamp 
$h_\mathrm{time*}$ of hashtags $h \in H^t$.  Formally, we define the following:
\begin{align*}
H^t_\mathrm{train} & = \{ h | h \in H^t \land h_\mathrm{time*} <    t_\mathrm{split} \} ,  \\
H^t_\mathrm{val}   & = \{ h | h \in H^t \land h_\mathrm{time*} \geq t_\mathrm{split} \} .
\end{align*}
Once we have split our hashtags into training and validation sets
according to $t_\mathrm{split}$, we next proceed to temporally split
our training documents $D$ into a training set $D^t_\mathrm{train}$ and a validation set
$D^t_\mathrm{val}$ for topic $t$ based on the posting
time stamp $d_{i,\mathrm{time*}}$ of each document $d_i$ as follows: 
%ormally, given $H^t$, we
%can label each document $d_i$ (containing positive features $D_i^+$) as follows:
\begin{align*}
D^t_\mathrm{train} & = \{ d_i | d_i \in D \land d_{i,\mathrm{time*}} <    t_\mathrm{split} \} ,  \\
D^t_\mathrm{val}   & = \{ d_i | d_i \in D \land d_{i,\mathrm{time*}} \geq t_\mathrm{split} \} .
\end{align*}
Then for $s \in \{ \mathrm{train}, \mathrm{val} \}$, we use the respective
hashtag sets $H^t_\mathrm{train}$ and $H^t_\mathrm{val}$ for labeling each $d_{i}^{t} \in D^t_s$:
\begin{align*}
d_{i}^{t} & =
  \begin{cases}
    1: \exists_{h \in H^t_s} \; h \in D_i^+ \\
    0: \mathrm{otherwise}
  \end{cases} .
\end{align*}
The critical insight here is that we not only divide the train and validation
temporally, but we divide the hashtag labels temporally and label the validation
data with an entirely disjoint set of topical labels from the training data.
%Selection of a set of documents and temporally splitting them into
%train and validation documents. The split is based on a split-time
%defined on hashtag set $H^{t}$ to preserve enough number of hashtags
%in train and validation sets.
The purpose behind this training and validation data split
and labeling is to ensure that learning hyperparameters are tuned so as
to prevent overfitting and maximize generalization to unseen topical
content (i.e., new hashtags).
\item {\bf Training and hyper-parameter tuning:}
Once $D^t_\mathrm{train}$ and $D^t_\mathrm{val}$ have been constructed,
we proceed to train our scoring function $f$ on $D^t_\mathrm{train}$ and
select hyperparameters to optimize Mean Average Precision (MAP) on
$D^t_\mathrm{val}$.  Once the optimal $f$ is found for $D_\mathrm{val}$,
we return it as our final learned topical scoring function for topic $t$.
%\item {\bf Learning: The weight vector $W$ is learned with classification method $M$ on the selected set of documents using tuned hyper parameters
\end{enumerate}
% Note that formal performance evaluation provided in experimental section.
Having defined our topical social sensor learning paradigm, it now remains
to empirically evaluate this methodology in a social media setting, which
we describe next.

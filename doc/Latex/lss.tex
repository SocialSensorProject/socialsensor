% This section covers the *formal* framework for learning topical social sensors

% =====================
% What are we doing?  
% (1) Formal problem setup and notation definitions.  Corpus of documents, features, labels, classification problem definition (for a generic classifier).
% (2) How we label data.
% (3) How we train (validation set critical for hyperparameters, what metric used for selection?).
% Note that formal performance evaluation provided in experimental section.
% =====================

% (1) Formal problem setup and notation definitions.  Corpus of documents, features, labels, classification problem definition (for a generic classifier).
%TODO: Formal learning framework.
We define a corpus of $N$ documents as $D = \{{d_{1}}, {d_{2}}, ..., {d_{N}} \}$, a set of $K$ topics as $T = \{{t_{1}}, {t_{2}}, ..., {t_{K}} \}$, a set of $M$ features as $F = \{0,1\}^{M}$, and a weight vector of $W \in \Re^{M}$ corresponding to each feature. 

For a given document ${d_{i}}$, features are defined as boolean vector ${F_{d_{i}}} \in \{0,1\}^{M}$ representing the presence of each single feature ${f_{i}} \in F$ in the document. The goal is to learn a classifier $\gamma$ that maps documents to a topic, thus learning the weight vector $W$ to be used for scoring documents at experiment time. For the classification method of $M$, the classifier is defined

\begin{equation}
(\gamma, M) : D \to T 
\end{equation}

\begin{equation}
t^{*} = argMin_{w} L(t,\hat{t})
\end{equation}

Where ${L : T \times T \to \Re_{+} }$ is the loss function indicating the penalty for an incorrect prediction and ${L(t,\hat{t})}$ is the loss for prediction of ${\hat{t}}$ instead of actual topic $t$.
%SCORING TWEETS AT TEST TIME
%Each document ${d_{i}} \in D$ is scored for a given topic ${t \in \{ T \}}$ by the measure of it's similarity to the topic defined as
%
%\begin{equation}
%Sim({d_{i}}, t) = \sum_{j} F_{d_{i}}^{j}) \times {w_{j}}
%\end{equation}
%
%where $F_{d_{i}}^{j}$ represents the $j$th value in $F_{d_{i}}$.

% (2) How we label data.
One critical bottleneck for learning targeted topical social sensors
is to achieve sufficient supervised content labeling.  With data
requirements often in the thousands of labels to ensure effective
learning and generalization over a large candidate feature space (as
found in social media), manual labeling is simply too time-consuming
for many users and crowdsourced labels are both costly and prone to
misinterpretation of users' information needs.  Fortuitously, hashtags
have emerged in recent years as a pervasive topical proxy on social
media sites --- hashtags originated on IRC chat, were adopted later
(and perhaps most famously) on Twitter, and now appear on other social
media platforms such as Instagram, Tumblr, and Facebook.  Hence as a
simple enabling insight that serves as a catalyst for effective
topical social sensor learning, we leverage a (small) set of
user-curated topical hashtags to efficiently provide a large number of
supervised topic labels for social media content.

%TODO: Formal math for defining the set of positive and negative labeled tweets via the hashtag set $H$.
Having the curated hashtag set $H^{t} \in \{h_{1}, h_{2},...,h_{l_{t}} \}$ for topic $t$, the label of document $d_{i}$ is defined as positive (topical) or negative (non-topical) by the following rule:

\begin{equation}
d_{i}^{t} = \exists _{h \in H^{t}} F_{d_{i}}^{h} = 1
\end{equation}

Where $d_{i}^{t}$ is the label of document $d_{i}$ and $F_{d_{i}}^{h}$ represents the $h$-th value in $F_{d_{i}}$.

% (3) How we train (validation set critical for hyperparameters, what metric used for selection?).
With the data labeling bottleneck resolved, we proceed to train
supervised classification and ranking methods to learn topical content
from a large feature space of source users and their locations, terms,
hashtags, and mentions. The training process includes the following steps:%TODO: An simple enumeration of the training steps?

\begin{enumerate}
\item Preprocess: Selection of a set of documents and splitting them into train and validation tweets. The split is based on a split-time defined on hashtag set $H^{t}$ to preserve enough number of hashtags in train and validation sets.
\item Hyper-parameter tuning: hyper-parameters are tuned on validation set of tweets. It is important to note that we remove tweets containing train hashtags from the set of validation tweets and perform the analysis on the remaining tweets, labeled by validation hashtags.
\item Learning: The weight vector $W$ is learned with classification method $M$ on the selected set of documents using tuned hyper parameters
\end{enumerate}
% Note that formal performance evaluation provided in experimental section.
% This section covers the *formal* framework for learning topical social sensors

% =====================
% What are we doing?  
% (1) Formal problem setup and notation definitions.  Corpus of documents, features, labels, classification problem definition (for a generic classifier).
% (2) How we label data.
% (3) How we train (validation set critical for hyperparameters, what metric used for selection?).
% Note that formal performance evaluation provided in experimental section.
% =====================

% (1) Formal problem setup and notation definitions.  Corpus of documents, features, labels, classification problem definition (for a generic classifier).
%TODO: Formal learning framework.
We define a corpus of $N$ documents as $D = \{{d_{1}}, {d_{2}}, ..., {d_{N}} \}$, a set of $M$ features as $F = \{{f_{1}}, {f_{2}}, ..., {f_{M}} \}$, a set of $K$ topics as $T = \{{t_{1}}, {t_{2}}, ..., {t_{K}} \}$, and a weight vector of $W = \{ {w_{1}}, {w_{2}}, ..., {w_{M}} \}$ corresponding to each feature. 

For a given document ${d_{i}}$, features are defined as boolean/real valued vector ${F_{d_{i}}} \in \Re^{M}$ representing the presence/frequency of each single feature ${f_{i}} \in F$. The goal is to learn a classifier $\gamma$ that maps documents to a topic, thus learning the weight vector $W$ to be used for scoring documents at experiment time. For the classification method of $M$, the classifier is defined

\begin{equation}
\gamma : D \to T 
\end{equation}

\begin{equation}
t^{*} = argMin_{w} L(t,\hat{t})
\end{equation}

Where ${L : T \times T \to \Re_{+} }$ is the loss function indicating the penalty for an incorrect prediction and ${L(t,\hat{t})}$ is the loss for prediction of ${\hat{t}}$ instead of actual topic $t$.

Each document ${d_{i}} \in D$ is scored for a given topic ${t \in \{ T \}}$ by the measure of it's similarity to the topic defined as

\begin{equation}
Sim({d_{i}}, t) = \sum_{j} F_{j}(d_{i}) \times {w_{j}}
\end{equation}

Afterwards, we present the ranked list of documents, ranked based on their similarity values.

% (2) How we label data.
One critical bottleneck for learning targeted topical social sensors
is to achieve sufficient supervised content labeling.  With data
requirements often in the thousands of labels to ensure effective
learning and generalization over a large candidate feature space (as
found in social media), manual labeling is simply too time-consuming
for many users and crowdsourced labels are both costly and prone to
misinterpretation of users' information needs.  Fortuitously, hashtags
have emerged in recent years as a pervasive topical proxy on social
media sites --- hashtags originated on IRC chat, were adopted later
(and perhaps most famously) on Twitter, and now appear on other social
media platforms such as Instagram, Tumblr, and Facebook.  Hence as a
simple enabling insight that serves as a catalyst for effective
topical social sensor learning, we leverage a (small) set of
user-curated topical hashtags to efficiently provide a large number of
supervised topic labels for social media content.

%TODO: Formal math for defining the set of positive and negative labeled tweets via the hashtag set $H$.
Using the curated hashtag set $H^{t}$ for topic $t$, a document is labeled as positive (topical) or negative (non-topical) by

\begin{equation}
L(d_{i}) = \left\{\begin{matrix}
1 & F_{d_{i}} \cdot H^{t} > 0  \\ 
0 & F_{d_{i}} \cdot H^{t} = 0
\end{matrix}\right.
\label{eq:labeling}
\end{equation}

Where $L(d_{i}) = 1$ shows that $d_{i}$ is a topical document and $L(d_{i}) = 0$ shows that $d_{i}$ is a non-topical document.

% (3) How we train (validation set critical for hyperparameters, what metric used for selection?).
With the data labeling bottleneck resolved, we proceed to train
supervised classification and ranking methods to learn topical content
from a large feature space of source users and their locations, terms,
hashtags, and mentions.

TODO: An simple enumeration of the training steps?

% Note that formal performance evaluation provided in experimental section.

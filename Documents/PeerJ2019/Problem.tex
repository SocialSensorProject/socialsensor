%!TEX root = main.tex

% =====================
% What are we doing?  
% (1) Formal problem setup and notation definitions.  Corpus of documents, features, labels, classification problem definition (for a generic classifier).
% (2) How we label data.
% (3) How we train (validation set critical for hyperparameters, what metric used for selection?).
% Note that formal performance evaluation provided in experimental section.
% =====================

% (1) Formal problem setup and notation definitions.  Corpus of documents, features, labels, classification problem definition (for a generic classifier).
%TODO: Formal learning framework.

  
%
%To this end, we now outline a methdology for training and evaluating such topic classifiers.
%
%{\color{red}
%In this section, we reduce the problem of learning topical content from large space of features and small set of examples provided by the user to the following setting that will match standard supervised learning paradigm.
%
%Here, the problem statement is that the user has an information need for high-precision topical content from Twitter. 
%The first step for the user is that he/she must provide labeled data to represent this information need for use in a targeted supervised learning setting.
% We assume that for each topic, the user will provide us with a set of hashtags. For example for the topic of \textit{Natural Disaster}, 
%the user will give us \{\#earthquake,\#flood,\#prayforthephilippines, ...\}. The goal is, given this topic and related hashtags, return a ranked
% list of tweets to the user that are highly relevant to the topic and match users information needs; meaning instead of returning a set of tweets
% that only match "disaster" or "natural", we realize the actual information need behind the searched topic and present all tweets matching this need. 
%To this end, we need a methodology that learns from the set of hashtags provided by users on how to pick up sensors (i.e., useful terms, mentions, 
%hashtags, locations, and users) and weight them to ensure picking new, unseen topical hashtags in future tweets. The following discussion intends
% to answer how to develop such methodology. 
%
%To this end, we assume a birth-time for each hashtag as the first time it has been used in our dataset. Assuming a birth-time, then we consider temporal split of our chosen set of hashtags and learn on one set and test on the other more recent set to evaluate the methods generalizability. The limitation of this method is since we use hashtags as topical proxies, we mislabel the tweets without hashtags. Therefore, we miss part of topical content that we could learn from for the price of automatic labeling of all tweets with minimal user effort. Now, we move on to mathematically formalize this problem.}
% }
% 


Our objective in this paper is to carry out a longitudinal study of topic classifiers for Twitter.
For each Twitter topic, we seek to build a binary classifier that can label
a previously unseen tweet as topical (or not). 
To achieve this, we  train and evaluate the classifier on a set of topically labeled historical tweets as described later in this article.

Formally, given an arbitrary tweet $d$ (a document in text classification parlance) 
and a set of topics $T = \{
t_1,\ldots,t_K\}$, we wish to train a scoring function $f^t: D \rightarrow \mathbb{R}$
for each topic $t \in T$ over a subset of labeled training tweets from $D = \{
d_1,\ldots,d_N \}$. \textcolor{red}{  We assume that each tweet $d_i \in D$ is represented by a vector of $m$ features $d_i=[d_i^1,\ldots,d_i^M]$ with $d_i^M \in \{0,1\}$ to indicate that the feature $M$ is associated with $d_i$ (1) or not (0),  and its associated label $t(d_i) \in \{
0,1 \}$ to indicate whether the tweet $d_i$ is topical (1) or not (0).  
As in any standard classification task, we wish to learn the mapping function $ f^t(d_i)$ so that it can be used to predict the label of a new unseen tweet $d_i$ with a high accuracy.}
%In an ideal scenario, we could train for topic $t$ 
%so that $\forall d_i \; f^t(d_i) = t(d_i)$ --- our scoring
%function (or more generally some threshold on it yielding a $\{0,1\}$ prediction) agrees perfectly with the topic
%labels of all tweets.  

%There are two catches that make our training setting somewhat
%non-standard and which underlie key difficulties when training topical
%classifiers for Twitter:  (1) Manually labeling documents is time-consuming so
%we need a way to manually label a large number of tweets with minimal
%user curation effort; \emph{We achieve this by leveraging the insight of
%of~\cite{lin2011smoothing} and use a set of predefined hashtags as topical proxies.}
%(2) We need to train our social sensor on
%known topical content, but tune it on novel topical validation content 
%that ensures the tuning achieves optimal generalization; \emph{We achieve this by 
%excising training content from our validation data so that our scoring
%function hyperparameter tuning ensures generalization.}
%We elaborate on these details next.

%\begin{equation}
%(\gamma, M) : D \to T 
%\end{equation}
%\part{title}
%\begin{equation}
%t^{*} = argMin_{w} L(t,\hat{t})
%\end{equation}
%
%Where ${L : T \times T \to \Re_{+} }$ is the loss function indicating the penalty for an incorrect prediction and ${L(t,\hat{t})}$ is the loss for prediction of ${\hat{t}}$ instead of actual topic $t$.

%SCORING TWEETS AT TEST TIME
%Each document ${d_{i}} \in D$ is scored for a given topic ${t \in \{ T \}}$ by the measure of it's similarity to the topic defined as
%
%\begin{equation}
%Sim({d_{i}}, t) = \sum_{j} F_{d_{i}}^{j}) \times {w_{j}}
%\end{equation}
%
%where $F_{d_{i}}^{j}$ represents the $j$th value in $F_{d_{i}}$.

% (2) How we label data.

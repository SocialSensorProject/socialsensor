%!TEX root = main.tex

\textcolor{red}{In this section, we describe the formal framework we use for our longitudinal study of topical classifiers. Hence, we first describe how we propose to label the data using a set of hand-curated user hashtags. Then, we proceed to describe the way we propose to split the dataset into train, validation and test sets, which is critical for such a longitudinal study of topical classifiers. 
Finally, we provide a brief description of several classification algorithms we use in our analysis.
}

% =====================
% What are we doing?  
% (1) Formal problem setup and notation definitions.  Corpus of documents, features, labels, classification problem definition (for a generic classifier).
% (2) How we label data.
% (3) How we train (validation set critical for hyperparameters, what metric used for selection?).
% Note that formal performance evaluation provided in experimental section.
% =====================

% (1) Formal problem setup and notation definitions.  Corpus of documents, features, labels, classification problem definition (for a generic classifier).
%TODO: Formal learning framework.

  
%
%To this end, we now outline a methdology for training and evaluating such topic classifiers.
%
%{\color{red}
%In this section, we reduce the problem of learning topical content from large space of features and small set of examples provided by the user to the following setting that will match standard supervised learning paradigm.
%
%Here, the problem statement is that the user has an information need for high-precision topical content from Twitter. 
%The first step for the user is that he/she must provide labeled data to represent this information need for use in a targeted supervised learning setting.
% We assume that for each topic, the user will provide us with a set of hashtags. For example for the topic of \textit{Natural Disaster}, 
%the user will give us \{\#earthquake,\#flood,\#prayforthephilippines, ...\}. The goal is, given this topic and related hashtags, return a ranked
% list of tweets to the user that are highly relevant to the topic and match users information needs; meaning instead of returning a set of tweets
% that only match "disaster" or "natural", we realize the actual information need behind the searched topic and present all tweets matching this need. 
%To this end, we need a methodology that learns from the set of hashtags provided by users on how to pick up sensors (i.e., useful terms, mentions, 
%hashtags, locations, and users) and weight them to ensure picking new, unseen topical hashtags in future tweets. The following discussion intends
% to answer how to develop such methodology. 
%
%To this end, we assume a birth-time for each hashtag as the first time it has been used in our dataset. Assuming a birth-time, then we consider temporal split of our chosen set of hashtags and learn on one set and test on the other more recent set to evaluate the methods generalizability. The limitation of this method is since we use hashtags as topical proxies, we mislabel the tweets without hashtags. Therefore, we miss part of topical content that we could learn from for the price of automatic labeling of all tweets with minimal user effort. Now, we move on to mathematically formalize this problem.}
% }
% 

%In an ideal scenario, we could train for topic $t$ 
%so that $\forall d_i \; f^t(d_i) = t(d_i)$ --- our scoring
%function (or more generally some threshold on it yielding a $\{0,1\}$ prediction) agrees perfectly with the topic
%labels of all tweets.  

%There are two catches that make our training setting somewhat
%non-standard and which underlie key difficulties when training topical
%classifiers for Twitter:  (1) Manually labeling documents is time-consuming so
%we need a way to manually label a large number of tweets with minimal
%user curation effort; \emph{We achieve this by leveraging the insight of
%of~\cite{lin2011smoothing} and use a set of predefined hashtags as topical proxies.}
%(2) We need to train our social sensor on
%known topical content, but tune it on novel topical validation content 
%that ensures the tuning achieves optimal generalization; \emph{We achieve this by 
%excising training content from our validation data so that our scoring
%function hyperparameter tuning ensures generalization.}
%We elaborate on these details next.

%\begin{equation}
%(\gamma, M) : D \to T 
%\end{equation}
%\part{title}
%\begin{equation}
%t^{*} = argMin_{w} L(t,\hat{t})
%\end{equation}
%
%Where ${L : T \times T \to \Re_{+} }$ is the loss function indicating the penalty for an incorrect prediction and ${L(t,\hat{t})}$ is the loss for prediction of ${\hat{t}}$ instead of actual topic $t$.

%SCORING TWEETS AT TEST TIME
%Each document ${d_{i}} \in D$ is scored for a given topic ${t \in \{ T \}}$ by the measure of it's similarity to the topic defined as
%
%\begin{equation}
%Sim({d_{i}}, t) = \sum_{j} F_{d_{i}}^{j}) \times {w_{j}}
%\end{equation}
%
%where $F_{d_{i}}^{j}$ represents the $j$th value in $F_{d_{i}}$.

% (2) How we label data.

\subsection*{Dataset labelling}

A critical bottleneck for learning targeted topical social classifiers 
is to achieve sufficient supervised content labeling.  With data
requirements often in the thousands of labels to ensure effective
learning and generalization over a large candidate feature space (as
found in social media), manual labeling is simply too time-consuming
for many users, while crowdsourced labels are both costly and prone to
misinterpretation of users' information needs.  Fortuitously, hashtags
have emerged in recent years as a pervasive topical proxy on social
media sites --- hashtags originated on Internet Relay Chat (IRC), were adopted later
(and perhaps most famously) on Twitter, and now appear on other social
media platforms such as Instagram, Tumblr, and Facebook.  Following
the approach of~\cite{lin2011smoothing}, for each topic $t \in T$, we leverage a (small) set of
user hand-curated topical hashtags $H^t$ to efficiently label a large number of
supervised topic labels for social media content.  

Specifically, we manually curated a broad thematic range of 10 topics shown in the top row of
Table~\ref{table:sampleHashtags} by annotating hashtag sets $H^t$ for each topic $t \in T$.  We used 4
independent annotators to query the Twitter search API to identify
candidate hashtags for each topic, requiring an inner-annotator
agreement of 3 annotators to permit a hashtag to be assigned to a
topic set. 





\subsection*{Dataset splitting}


We now provide a procedure for labeling data with $H^t$ for training, validation and test.
Following this, we proceed to train supervised classification and ranking methods to learn topical content from a large feature space -- this feature space includes terms, hashtags, mentions, authors and their locations. \textcolor{red}{ In the following, we describe three key points related to the temporal splitting of the dataset, sampling negative examples, and hyper-parameter tuning.}


%TODO: An simple enumeration of the training steps?
 \subfour{Temporally split for train, validation and test using $H^t$:}
As standard for machine learning methods, we divide our training data into
train, validation, and test sets --- the validation set is used for hyperparameter tuning to control
overfitting and ensure generalization to unseen data.  
As a critical insight for topical generalization where we view correct classification 
of tweets with \emph{previously unseen topical hashtags} as a proxy for topical generalization, 
we do not simply
split our data temporally into train and test sets and label both with \emph{all} 
hashtags in $H^t$. \textcolor{red}{ Rather,
we split each $H^t$ into three disjoint sets $H^t_\mathrm{train}$, $H^t_\mathrm{val}$, and $H^t_\mathrm{test}$
according to two time stamps $t^\mathrm{train}_\mathrm{split}$ and $t^\mathrm{val}_\mathrm{split}$ for topic $t$ and the first usage time stamp 
$h_\mathrm{time*}$ of each hashtag $h \in H^t$.  In short, all hashtags $h \in H^t$ first used
before $t^\mathrm{train}_\mathrm{split}$ are used to generate positive labels in the training data, all hashtags $h \in H^t$ first used
after $t^\mathrm{train}_\mathrm{split}$ and before $t^\mathrm{val}_\mathrm{split}$ 
are used to generate positive labels in the validation data,  and
the remaining hashtags are used to generate positive labels in the test data.}

\noindent To achieve this effect formally, we define the following:
\begin{align*}
H^t_\mathrm{train} & = \{ h | h \in H^t \land h_\mathrm{time*} <    t^\mathrm{train}_\mathrm{split} \} ,  \\
H^t_\mathrm{val}   & = \{ h | h \in H^t \land h_\mathrm{time*} \geq t^\mathrm{train}_\mathrm{split}  \land h_\mathrm{time*} < t^\mathrm{val}_\mathrm{split} 	 \},  \\
H^t_\mathrm{test}   & = \{ h | h \in H^t \land h_\mathrm{time*} \geq t^\mathrm{val}_\mathrm{split} \} .
\end{align*}


Once we have split our hashtags into training and validation sets
according to $t^\mathrm{train}_\mathrm{split}$ and $t^\mathrm{val}_\mathrm{split}$, we next proceed to temporally split
our training documents $D$ into a training set $D^t_\mathrm{train}$, a validation set
$D^t_\mathrm{val}$, and a test set $D^t_\mathrm{test}$ for topic $t$ based on the posting
time stamp $d_{i,\mathrm{time*}}$ of each tweet $d_i$ as follows: 
%ormally, given $H^t$, we
%can label each document $d_i$ (containing positive features $D_i^+$) as follows:
\begin{align*}
D^t_\mathrm{train} & = \{ d_i | d_i \in D \land d_{i,\mathrm{time*}} <    t^\mathrm{train}_\mathrm{split} \} ,  \\
D^t_\mathrm{val}   & = \{ d_i | d_i \in D \land d_{i,\mathrm{time*}} \geq t^\mathrm{train}_\mathrm{split} \land  d_{i,\mathrm{time*}} <    t^\mathrm{val}_\mathrm{split} \land (\forall h \in d_i: h \notin H^t_\mathrm{train}) \} ,  \\
D^t_\mathrm{test}   & = \{ d_i | d_i \in D \land d_{i,\mathrm{time*}} \geq t^\mathrm{train}_\mathrm{val} \land (\forall h \in d_i: h \notin H^t_\mathrm{train}) \} .
\end{align*}


The critical insight here is that we do not only divide the train, validation, and test
temporally, but we also divide the hashtag labels temporally and label the validation and test
data with an entirely disjoint set of topical labels from the training data. \textcolor{red}{Moreover, in order to avoid overfitting and allow a better generalization, we argue that removing tweets containing training hashtags is essential to avoid training a classifier that simply memorizes training hashtags.}
 



%\textcolor{red}{Next we define the set of positive tweet examples formally as} $D_i^+$ and note that
%$D_i^+$ may include feature IDs for the content of $d_i$ (e.g., terms and, importantly, 
%hashtags) as well as its meta-data (e.g., author, location).
Finally, to label the train, validation, and test data sets $D^t_\mathrm{train}$, $D^t_\mathrm{val}$ and $D^t_\mathrm{test}$, 
we use the respective
hashtag sets $H^t_\mathrm{train}$, $H^t_\mathrm{val}$, $H^t_\mathrm{test}$ for generating
the topic label for a particular tweet $t(d_{i}) \in \{0,1\}$ as follows:
\begin{align*}
t(d_{i}) & =
  \begin{cases}
    1 \textrm{ if }  \exists \; h \in d_i:  h  \in H^t  \\
    0  \textrm{ otherwise}
  \end{cases} .
\end{align*}
%To recap the methodology, we specify a set of hashtags as a proxy for a topic.  We split
%those hashtags into train and validation sets according to a time of first usage and fixed time point.
%We split the tweet data into train and validation sets according to the same fixed time point.
%Finally, we label a tweet in the train (validation) data set as positive if it contains
%a hashtag in the train (validation) hashtag set.  

\begin{table*}[t!]
\centering
\caption{Train/Validation/Test Hashtag samples and statistics.}
{\def\arraystretch{1.2}
\resizebox{\textwidth}{!}{%
% Preview source code for paragraph 0

% Preview source code for paragraph 0
% Preview source code for paragraph 0

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline 
\multirow{2}{*}{} & \multirow{2}{*}{\textbf{Tennis}} & \multirow{2}{*}{\textbf{Space}} & \multirow{2}{*}{\textbf{Soccer}} & \multirow{2}{*}{\textbf{Iran Deal}} & \textbf{Human} & \textbf{Celebrity} & \textbf{Social} & \textbf{Natural} & \multirow{2}{*}{\textbf{Epidemics}} & \multirow{2}{*}{\textbf{LGBT}}\tabularnewline
 &  &  &  &  & \textbf{Disaster} & \textbf{Death} & \textbf{Issues} & \textbf{Disaster} &  & \tabularnewline
\hline 
\hline 
\textbf{\#TrainHashtags} & 62 & 112 & 144 & 12 & 57 & 33 & 37 & 61 & 55 & 30\tabularnewline
\hline 
\textbf{\#ValHashtags} & 14 & 32 & 42 & 2 & 8 & 4 & 5 & 4 & 17 & 9\tabularnewline
\hline 
\textbf{\#TestHashtags} & 14 & 17 & 21 & 3 & 12 & 7 & 8 & 17 & 13 & 5\tabularnewline
\hline 
\hline 
\textbf{\#+TrainTweets} & 21,716 & 5,333 & 14,006 & 6,077 & 153,612 & 155,121 & 27,423 & 46,432 & 14,177 & 1,344\tabularnewline
\hline 
\textbf{\#-TrainTweets} & 191,905 & 46,587 & 123,073 & 54,045 & 1,363,260 & 1,376,872 & 244,106 & 411,609 & 125,092 & 11,915\tabularnewline
\hline 
\hline 
\textbf{\#+ValTweets} & 884 & 2,281 & 4,073 & 1,261 & 53,340 & 23,710 & 3,088 & 843 & 4,348 & 50\tabularnewline
\hline 
\textbf{\#-ValTweets} & 7,860 & 20,368 & 36,341 & 11,363 & 473,791 & 210,484 & 27,598 & 7,456 & 39,042 & 443\tabularnewline
\hline 
\hline 
\textbf{\#+TestTweets} & 1,510 & 5,908 & 11,503 & 368 & 34,055 & 7,334 & 14,566 & 5,240 & 3,105 & 692\tabularnewline
\hline 
\textbf{\#-TestTweets} & 13,746 & 53,348 & 103,496 & 3,256 & 305,662 & 65,615 & 130,118 & 47,208 & 27,828 & 6,325\tabularnewline
\hline 
\hline 
 & \#usopenchampion & \#asteroids & \#worldcup & \#irandeal & \#gazaunderattack & \#robinwilliams & \#policebrutality & \#earthquake & \#ebola & \#loveislove\tabularnewline
\cline{2-11} \cline{3-11} \cline{4-11} \cline{5-11} \cline{6-11} \cline{7-11} \cline{8-11} \cline{9-11} \cline{10-11} \cline{11-11} 
\textbf{Sample} & \#novakdjokovic & \#astronauts & \#lovesoccer & \#iranfreedom & \#childrenofsyria & \#ripmandela & \#michaelbrown & \#storm & \#virus & \#gaypride\tabularnewline
\cline{2-11} \cline{3-11} \cline{4-11} \cline{5-11} \cline{6-11} \cline{7-11} \cline{8-11} \cline{9-11} \cline{10-11} \cline{11-11} 
\textbf{Hashtags} & \#wimbledon & \#satellite & \#fifa & \#irantalk & \#iraqwar & \#ripjoanrivers & \#justice4all & \#tsunami & \#vaccine & \#uniteblue\tabularnewline
\cline{2-11} \cline{3-11} \cline{4-11} \cline{5-11} \cline{6-11} \cline{7-11} \cline{8-11} \cline{9-11} \cline{10-11} \cline{11-11} 
 & \#womenstennis & \#spacecraft & \#realmadrid & \#rouhani & \#bombthreat & \#mandela & \#freetheweed & \#abfloods & \#chickenpox & \#homo\tabularnewline
\cline{2-11} \cline{3-11} \cline{4-11} \cline{5-11} \cline{6-11} \cline{7-11} \cline{8-11} \cline{9-11} \cline{10-11} \cline{11-11} 
 & \#tennisnews & \#telescope & \#beckham & \#nuclearpower & \#isis & \#paulwalker & \#newnjgunlaw & \#hurricanekatrina & \#theplague & \#gaymarriage\tabularnewline
\hline 
\end{tabular}

}
}
\label{table:sampleHashtags}
\end{table*}


The purpose behind this training, validation and test data split
and labeling is to ensure that learning hyper-parameters are tuned so as
to prevent overfitting and maximize generalization to unseen topical
content (i.e., new hashtags).
We remark that \emph{a classifier that simply
memorizes training hashtags will fail to correctly classify the validation data} except in 
cases where a tweet contains both a training and validation hashtag. 

Per topic, hashtags were split into train and test sets
according to their first usage time stamp roughly according to a 3/5
to 2/5 proportion (the test interval spanned between 9-14 months).  
The train set was further temporally subdivided
into train and validation hashtag sets according to a 5/6 to 1/6
proportion.  We show a variety of statistics and five sample hashtags
per topic in Table~\ref{table:sampleHashtags}.  Here we can see that
different topics had varying prevalence in the data
with \textit{Soccer} being the most tweeted topic
and \textit{IranDeal} being the least tweeted according to our curated
hashtags.


\subfour{Sampling negative examples:} \textcolor{red}{ Topic classification is very often an unbalanced classification task, since usually, there are much more negative examples than positive examples. Indeed, the large number of users on twitter, their diversity, their wide range interests, and the short lifetime of topics discussed on a daily basis make that  diverse topics are discussed on a short time period, thus having only a small set of positive examples for each topic. For example, the open directory project (ODP also known as DMOZ), which provides a hierarchically classification structure of roughly 5M Web ressources over 1M topics\footnote{http://www.geniac.net/odp/}. Therefore, given the huge amount of negative examples in a real classification scenario (e.g., roughly 800 million negative examples against 500k positive examples for the human disaster topic), to efficiently tune each classifier, we have chosen to sample negative examples such that positive examples represent 10\% of the dataset and the negative examples represent 90\% of the dataset. This rule is valid for the training, validation and test sets of each topic.
}




\subfour {Training and hyper-parameter tuning:}
Once $D^t_\mathrm{train}$ and $D^t_\mathrm{val}$ have been constructed,
we proceed to train our scoring function $f^t$ on $D^t_\mathrm{train}$ and
select hyperparameters to optimize Average Precision (AP)~\cite{manning_ir} (a ranking
metric) on
$D^t_\mathrm{val}$.  Once the optimal $f^t$ is found for $D^t_\mathrm{val}$,
we return it as our final learned topical scoring function $f^t$ for topic $t$.
Because $f^t(d_i) \in \mathbb{R}$ is a scoring function, it can be used to rank.
%\item {\bf Learning: The weight vector $W$ is learned with classification method $M$ on the selected set of documents using tuned hyper parameters
% Note that formal performance evaluation provided in experimental section.

%Analogously to train and validation data, test data is generated the same way.
%, but we omit formal notation (identical to the above) in order to reduce clutter.
%Test data has its own time stamp ($> t_\mathrm{split}$), its own set of
%temporally disjoint data, and its own set of hashtags 
%temporally disjoint from the training and validation data; this is done in order to evaluate
%generalization performance of the learned classifier on future data with topical hashtags
%not seen during training.    
With train, validation, and testing data defined along with the training methodology,
it remains now to extract relevant features, 
described next.




%!TEX root = main.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t!]
\vspace{-0.5mm}
\centering
\caption{Cutoff threshold and corresponding number of unique values of candidate features \textit{CF} for learning.}
{%\footnotesize
%\small
\def\arraystretch{1.2}
\begin{tabular}{|l|c|c|}
\hline
 & \textbf{Threshold} & \textbf{\#Unique Values} \\ \hline \hline
\textbf{From} & 235 &  206,084 \\ \hline
\textbf{Hashtag} & 65 & 201,204 \\ \hline
\textbf{Mention} & 230 & 200,051 \\ \hline
\textbf{Location} & 160 & 205,884 \\ \hline
\textbf{Term} & 200 & 204,712 \\ \hline
\hline
\textbf{Features (CF)} & - & 1,017,935 \\ \hline
\end{tabular}
}
\vspace{-1mm}
\label{table:learningFeatures}
\vspace{-1.5mm}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%How we curated hashtags: need a good story here.  Inner-annotator agreement of 3/4.
% With the formal definition of learning topical classifiers provided
% in Sec.~\hyperref[sec:lss]{Learning Topical Social Sensors} and the overview of our data in
% Sec.~\hyperref[sec:datasetStatistics]{Data Description}, we proceed to outline our
% experimental methodology on our Twitter corpus.  
 
%As noted in Sec.~\hyperref[sec:datasetStatistics]{Data Description},

\subsection*{Topic classification features}

\textcolor{red}{The set of features that we may consider for each tweet $d_i$ are: (i) \textit{User}, (ii) \textit{Mention}, (iii) \textit{Location}, (iv) \textit{Term}, and (v) \textit{Hashtag} features.}
%The summation of number of unique values of features shown in Table.~\ref{table:featureStatistics} results in a total number of $538,365,507$ features. As noted earlier, we are working on a $829,026,458$ tweet corpus. Thus with great number of features and tweets, there is a need for techniques to annotate the data and select a subset of features for learning. 
%
%We explained manual hashtag curation for each topic as a proxy for labeling the tweets. More specifically, the hashtag set $H^{t}$ for each topic $t \in T$ is curated with two annotators individually. Inner-annotator agreement is achieved by reviewing and merging these sets with two more individuals.  provides samples of hashtags, number of train hashtags, test hashtags, and topical tweets for each topic.
\textcolor{red}{Because we have a total of $538,365,507$ unique features in our
Twitter corpus (the summation unique values of features shown in Table~\ref{table:featureStatistics})}, it is critical to pare this down to a size amenable
for efficient learning and robust to overfitting.  To this end, we
thresholded all features according to the frequencies listed in
Table~\ref{table:learningFeatures}.  
\textcolor{red}{The rationale in our thresholding was initially that all features should have the same frequency cutoff and that we should keep roughly the same number of features for each category in order to achieve about 1 million features (roughly 250k features for each category).}  However, in  initial experimentation, we found that a high threshold pruned a large number of informative hashtag and locations.  \textcolor{red}{To this end, we lowered the threshold for hashtags and locations in order to get a reasonable number of features.}  We also removed common English stopwords which further reduced the unique term count.  Overall, we end up with $1,017,935$ candidate features (\textit{CF}) for learning topical classifiers.

%Regarding feature selection, it is impossible to learn a model on total number of $538,365,507$ features. To learn such a model would require a very large set of training samples, and, feature vectors would be extremely sparse considering $140$ characters limitation of Twitter. Therefore, we performed a primary feature selection based on frequency of each feature by:
%\begin{itemize}
%\item Cleaning the \textit{Term} feature to remove stop-words
%\item Choosing a cut-off threshold on the frequency of features
%\end{itemize} 
%This results in almost $1$ million features. The detailed values of cut-off thresholds and number of remaining unique values for each feature is shown in . Since \textit{Term} and \textit{Location} features exhibited fewer unique values in the corpus, we chose a lower threshold for these features.

%Train/validation/test split date selection -- temporally .5,.1,.4
%\label{label:split}
%In order to conduct our experiments, the train, validation and test set of tweets are formed by temporally dividing the dataset over $2$ years. Since our tweet labeling is through topical hashtags, this division is done in a way to preserve sufficient number of hashtags for train, validation, and test timespan. To this purpose, hashtags are divided based on their birthday with $50$ percent of hashtags born at train timespan, $10$ percent born at validation timespan, and the last $40$ percent born at test timespan.


%Feature selection: threshold per feature 159 and 50 (just explain rationale for lower hashtag and location thresholds).

\subsection*{Supervised Learning Algorithms}

With our labeled training, validation, and test datasets 
%defined in
%Sec.~\hyperref[sec:lss]{Learning Topical Social Sensors} 
and our candidate feature set \textit{CF} now defined, we proceed to apply different probabilistic classification and ranking
algorithms to generate a scoring function $f^t$ for learning topical classifiers 
as defined previously.
%Sec.~\hyperref[sec:lss]{Learning Topical Social Sensors}.  
In this paper, we experiment with 
the following five state-of-the-art supervised classification and ranking methods:
\textcolor{red}{
\begin{enumerate}
\item {\bf Logistic Regression (LR)}~\cite{liblinear}: LR uses logistic function to predict each tweet topics score/probability. We used L2 regularization with the parameter C (the inverse of regularization strength) selected from a search within the values $C \in \{10^{-12}, 10^{-11}, ..., 10^{11}, 10^{12}\}$.
\item {\bf Na\"{i}ve Bayes (NB)}~\cite{mccallum98nb}: NB makes an independence assumption between features (which is often inaccurate) to model the conditional probability. We used a Bayesian smoothing using Dirichlet priors with the parameter $\alpha$ selected from a search within the values $\alpha \in \{10^{-20}, 10^{-15}, 10^{-8}, 10^{-3}, 10^{-1}, 1\}$.
\item {\bf RankSVM}~\cite{largescale_ranksvm}: RankSVM is a variant of the support vector machine algorithm, which is used to solve certain ranking problems via learning to rank.  We used a linear kernel with the regularization parameter C (the trade-off between training error and margin) selected in the range $C \in \{10^{-12}, 10^{-11}, ..., 10^{11}, 10^{12}\}$.
\item {\bf Random Forest (RF)}~\cite{Breiman2001}: RF is an ensemble learning method for classification that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes of the individual trees. We mainly tuned the following hyper-parameters: the number of trees in the forest was selected from a search within the respective values $\{10, 20, 50, 100, 200\}$.
\item {\bf k-Nearest Neighbors (k-NN)}~\cite{Aha1991}: k-NN is a non-parametric method used for classification. An instance is classified by a plurality vote of its k neighbors, with the object being assigned to the class most common among its $k$ nearest neighbors. The value of $k$ was selected from a search within the respective values $\{1, 2, 3, 4, 5, 6, 7, 8, 9, 10\}$.
\end{enumerate}
}
%As outlined in the Methodology section, 
%Sec~\hyperref[sec:lss]{Learning Topical Social Sensors}, 
%tuning of hyperparameters on a validation
%dataset is critical.  In our experiments, we tune the following hyperparameters:
%\begin{itemize}
%\item \textit{Logistic Regression}: $L_2$ regularization constant $C$ is tuned for $C \in \{10^{-12}, 10^{-11}, ..., 10^{11}, 10^{12}\}$.
%\item \textit{Na\"{i}ve Bayes}: Dirichlet prior $\alpha$ is tuned for $\alpha \in \{10^{-20}, 10^{-15}, 10^{-8}, 10^{-3}, 10^{-1}, 1\}$.
%\item \textit{All Classfiers}: The number of top features $M$ selected based on their Mutual Information is tuned for $M \in \{10^2, 10^3, %10^4, 10^5\}$.
%\item \textcolor{red}{How about RankSVM parameters?}
%\end{itemize}
\textcolor{red}{We remark that almost all algorithms 
performed better with feature selection and hence we used feature
selection for all algorithms, where the number of top features $M$ was selected based on their Mutual Information in the range $M \in \{10^2, 10^3, 10^4, 10^5\}$. }
Hyperparameter tuning is done via exhaustive grid search
using the Average Precision
(AP)~\cite{manning_ir} ranking metric to select the best scoring function $f^t$ on the validation data.
Once found, $f^t$ can be applied to any tweet $d_i$ to provide a score $f^t(d_i)$
used to \emph{rank} tweets in the test data.
Code to process the raw Twitter data and to train and evaluate these classifiers as described above is provided on github.\footnote{\url{https://github.com/SocialSensorProject/socialsensor}}

\textcolor{red}{In the next section, we describe the outcome of the intensive evaluation we carried out for our longitudinal study of topic classification.}